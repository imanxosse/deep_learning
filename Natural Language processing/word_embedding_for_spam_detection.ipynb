{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "02_word_embedding_for_spam_detection.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MNfTYpAe7py",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "# Using word embeddings for spam detection\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c7COPvve7p0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import argparse\n",
        "import gensim.downloader as api\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjs8Gvcxe7p6",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Getting the data \n",
        "\n",
        "Getting the data The data for our model is available publicly and comes from the SMS spam collection dataset from the UCI Machine Learning Repository. The following code will download the file and parse it to produce a list of SMS messages and their corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9qrfaYae7p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_and_read(url):\n",
        "    local_file = url.split('/')[-1]\n",
        "    p = tf.keras.utils.get_file(local_file, url, \n",
        "        extract=True, cache_dir=\".\")\n",
        "    labels, texts = [], []\n",
        "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
        "    with open(local_file, \"r\") as fin:\n",
        "        for line in fin:\n",
        "            label, text = line.strip().split('\\t')\n",
        "            labels.append(1 if label == \"spam\" else 0)\n",
        "            texts.append(text)\n",
        "    return texts, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vziGGg5e7p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "texts, labels = download_and_read(DATASET_URL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBNhRi9ae7qD",
        "colab_type": "code",
        "outputId": "bdd55a98-c41d-4b43-e112-f71ec0f013c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "texts[1:3]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ok lar... Joking wif u oni...',\n",
              " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4OuCuWDe7qI",
        "colab_type": "code",
        "outputId": "5c98b532-0efc-4677-c3d3-17e0b69e10c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels[1:3]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxKsfHIOe7qM",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "## Tokenizing and padding text  \n",
        "\n",
        "\n",
        "- We will use the Keras tokenizer to convert each SMS text into a sequence of words, and then create the vocabulary using the **fit_on_texts()** method on the tokenizer.\n",
        "- We then convert the SMS messages to a sequence of integers using the **texts_to_sequences()**. Finally, since the network can only work with fixed length sequencesof integers, we call the **pad_sequences()** function to pad the shorter SMS messages with zeros.\n",
        "- The longest SMS message in our dataset has 189 tokens (words). In many applications where there may be a few outlier sequences that are very long, we would restrict the length to a smaller number by setting the maxlen flag. In that case, sentences longer than maxlen tokens would be truncated, and sentences shorter than maxlen tokens would be padded.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Wgmft5e7qN",
        "colab_type": "code",
        "outputId": "8dc6c9c5-460d-4aba-fa77-c817045f1f79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(texts)\n",
        "text_sequences = tokenizer.texts_to_sequences(texts) # convert SMS to integers\n",
        "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences) # pad the shorter SMS with zeros.\n",
        "\n",
        "num_records = len(text_sequences)\n",
        "max_seqlen = len(text_sequences[0]) # The length of all the text sequences is 189\n",
        "\n",
        "print(f\"{num_records:d} sentences, max length: {max_seqlen:d}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5574 sentences, max length: 189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqaLrdY9e7qR",
        "colab_type": "code",
        "outputId": "e8209b55-e699-4c2b-a4ec-97487f422cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "text_sequences[2]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,   47,  489,    8,   19,    4,  797,  901,\n",
              "          2,  176, 1941, 1105,  659, 1942, 2331,  261, 2332,   71, 1941,\n",
              "          2, 1943,    2,  337,  489,  555,  960,   73,  391,  174,  660,\n",
              "        392, 2997], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZMbS8nQe7qX",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "### Note:\n",
        "> We will also convert our labels to categorical or one-hot encoding format, because the loss function we would like to choose *(categorical cross-entropy)* expects to see the labels in that format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83VBz7KVe7qY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 2 # There are only two categories, spam or ham\n",
        "\n",
        "cat_labels = tf.keras.utils.to_categorical(labels, \n",
        "                                           num_classes=NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E89coObxe7qc",
        "colab_type": "code",
        "outputId": "cfbd1008-126d-4ec6-dc7b-9beb3a94b522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "cat_labels[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dinJ7f-Te7qg",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "The tokenizer allows access to the vocabulary created through the word_index attribute, which is basically a dictionary of vocabulary words to their index positions in the vocabulary. We also build the reverse index that enables us to go from index position to the word itself. In addition, we create entries for the PAD character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj7r1AMMe7qh",
        "colab_type": "code",
        "outputId": "5be75763-a4b5-447d-e8a1-648b2a3196e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# vocabulary\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "word2idx[\"PAD\"] = 0\n",
        "idx2word[0] = \"PAD\"\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "print(f\"vocab size: {vocab_size:d}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size: 9010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A84cFQdpe7qm",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "## Create dataset object\n",
        "Finally, we create the dataset object that our network will work with. The dataset object allows us to set up some properties, such as the batch size, declaratively. <br>\n",
        "Here, we build up a dataset from our padded sequence of integers and categorical labels, shuffle the data, and split it into training, validation, and test sets.<br> \n",
        "Finally, we set the batch size for each of the three datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm00lcdFe7qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
        "\n",
        "dataset = dataset.shuffle(10000)\n",
        "\n",
        "test_size = num_records // 4\n",
        "val_size = (num_records - test_size) // 10\n",
        "\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DvG9luXe7qt",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "## Building the embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W-AACrGe7qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_embedding_matrix(sequences, word2idx, embedding_dim, embedding_file):\n",
        "    EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
        "    if os.path.exists(embedding_file):\n",
        "        E = np.load(embedding_file)\n",
        "    else:\n",
        "        vocab_size = len(word2idx)\n",
        "        E = np.zeros((vocab_size, embedding_dim))\n",
        "        word_vectors = api.load(EMBEDDING_MODEL)\n",
        "        for word, idx in word2idx.items():\n",
        "            try:\n",
        "                E[idx] = word_vectors.word_vec(word)\n",
        "            except KeyError:   # word not in embedding\n",
        "                pass\n",
        "            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n",
        "            #     pass\n",
        "        np.save(embedding_file, E)\n",
        "    return E\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-whaFdUe7qx",
        "colab_type": "text"
      },
      "source": [
        "#### Observation\n",
        "- In order to keep our model size small, we want to only consider embeddings for words that exist in our vocabulary. This is done using the above code, which creates a smaller embedding matrix for each word in the vocabulary. Each row in the matrix corresponds to a word, and the row itself is the vector corresponding to the embedding for the word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWvw4Oawe7qy",
        "colab_type": "code",
        "outputId": "b85db02d-e8ee-4d05-f8b8-a8ce13e068a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "DATA_DIR = \"data\"\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "\n",
        "EMBEDDING_DIM = 300\n",
        "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
        "\n",
        "E = build_embedding_matrix(text_sequences, \n",
        "                           word2idx, \n",
        "                           EMBEDDING_DIM, \n",
        "                           EMBEDDING_NUMPY_FILE)\n",
        "\n",
        "print(\"Embedding matrix:\", E.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding matrix: (9010, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP-6sU_Qj3YP",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "## Define spam filter\n",
        "\n",
        "Depending on the run mode, that is, whether we will learn the embeddings from scratch, do transfer learning, or do fine-tuning, the Embedding layer in the network would be slightly different. \n",
        "\n",
        "- When the network starts with randomly initialized embedding weights (run_mode == \"scratch\"), and learns the weights during the training, we set the trainable parameter to True. \n",
        "\n",
        "- In the transfer learning case (run_mode == \"vectorizer\"), we set the weights from our embedding matrix E but set the trainable parameter to False, so it doesn't train.\n",
        "\n",
        "- In the fine-tuning case (run_mode == \"finetuning\"), we set the embedding weights from our external matrix E (in the data folder), as well as set the layer to trainable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ToNOAfde7q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpamClassifierModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_sz, embed_sz, input_length, num_filters, kernel_sz, output_sz, run_mode, embedding_weights, **kwargs):\n",
        "      super(SpamClassifierModel, self).__init__(**kwargs)\n",
        "      if run_mode == \"scratch\":\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                                                   embed_sz,\n",
        "                                                   input_length=input_length,\n",
        "                                                   trainable=True)\n",
        "      elif run_mode == \"vectorizer\":\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                                                   embed_sz,\n",
        "                                                   input_length=input_length,\n",
        "                                                   weights=[embedding_weights],\n",
        "                                                   trainable=False)\n",
        "      else:\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                                                   embed_sz,\n",
        "                                                   input_length=input_length,\n",
        "                                                   weights=[embedding_weights],\n",
        "                                                   trainable=True)\n",
        "      self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
        "\n",
        "      self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
        "                                         kernel_size=kernel_sz,\n",
        "                                         activation=\"relu\")\n",
        "      \n",
        "      self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
        "      \n",
        "      self.dense = tf.keras.layers.Dense(output_sz, \n",
        "                                         activation=\"softmax\")\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dense(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os4ixlSXe7q6",
        "colab_type": "code",
        "outputId": "7b4e11da-a9e0-4e40-9939-242b11be7910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "conv_num_filters = 256\n",
        "conv_kernel_size = 3\n",
        "\n",
        "# (run_mode == \"scratch\")    => Learns the weights during the training.\n",
        "# (run_mode == \"vectorizer\") => Transfer learning\n",
        "# (run_mode == \"finetuning\") => Fine tuning \n",
        "run_mode='vectorizer'\n",
        "\n",
        "\n",
        "\n",
        "model = SpamClassifierModel(vocab_size, \n",
        "                            EMBEDDING_DIM, \n",
        "                            max_seqlen, # 189 words\n",
        "                            conv_num_filters, \n",
        "                            conv_kernel_size, \n",
        "                            NUM_CLASSES,\n",
        "                            run_mode, E)\n",
        "\n",
        "model.build(input_shape=(None, max_seqlen))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"spam_classifier_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  2703000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              multiple                  230656    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  514       \n",
            "=================================================================\n",
            "Total params: 2,934,170\n",
            "Trainable params: 231,170\n",
            "Non-trainable params: 2,703,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngOKpPWznxdZ",
        "colab_type": "text"
      },
      "source": [
        "#### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQGCud6ke7q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4k93kHan0pB",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mAYkgvAe7rC",
        "colab_type": "code",
        "outputId": "455efda0-928c-4d6a-f8b0-bad8f4b35b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "CLASS_WEIGHTS = { 0: 1, 1: 8 }\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "model.fit(train_dataset, \n",
        "          epochs=NUM_EPOCHS, \n",
        "          validation_data=val_dataset, \n",
        "          class_weight=CLASS_WEIGHTS)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "29/29 [==============================] - 1s 24ms/step - loss: 0.5075 - accuracy: 0.8629 - val_loss: 0.1299 - val_accuracy: 0.9688\n",
            "Epoch 2/3\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 0.1948 - accuracy: 0.9601 - val_loss: 0.0678 - val_accuracy: 0.9896\n",
            "Epoch 3/3\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 0.1258 - accuracy: 0.9790 - val_loss: 0.0550 - val_accuracy: 0.9896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe2241bb390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dIyO0Xgni3z",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25UH2TNOe7rJ",
        "colab_type": "code",
        "outputId": "aa7fa023-b9e4-4cd1-c2e9-14d8be021d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "labels, predictions = [], []\n",
        "for Xtest, Ytest in test_dataset:\n",
        "    Ytest_ = model.predict_on_batch(Xtest)\n",
        "    ytest = np.argmax(Ytest, axis=1)\n",
        "    ytest_ = np.argmax(Ytest_, axis=1)\n",
        "    labels.extend(ytest.tolist())\n",
        "    predictions.extend(ytest.tolist())\n",
        "\n",
        "print(f\"test accuracy: {accuracy_score(labels, predictions):.3f}\")\n",
        "print(\"confusion matrix\")\n",
        "print(confusion_matrix(labels, predictions))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 1.000\n",
            "confusion matrix\n",
            "[[1116    0]\n",
            " [   0  164]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKK5Otl5oKSp",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    }
  ]
}