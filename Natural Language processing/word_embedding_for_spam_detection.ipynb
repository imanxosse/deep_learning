{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MNfTYpAe7py"
   },
   "source": [
    "<br>\n",
    "\n",
    "# Using word embeddings for spam detection\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2c7COPvve7p0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjs8Gvcxe7p6"
   },
   "source": [
    "<br>\n",
    "\n",
    "## Getting the data \n",
    "\n",
    "Getting the data The data for our model is available publicly and comes from the SMS spam collection dataset from the UCI Machine Learning Repository. The following code will download the file and parse it to produce a list of SMS messages and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9qrfaYae7p7"
   },
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, \n",
    "        extract=True, cache_dir=\".\")\n",
    "    labels, texts = [], []\n",
    "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == \"spam\" else 0)\n",
    "            texts.append(text)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vziGGg5e7p_"
   },
   "outputs": [],
   "source": [
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "texts, labels = download_and_read(DATASET_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "EBNhRi9ae7qD",
    "outputId": "7681635a-8b9d-4294-bbe0-ca5b9cd397db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ok lar... Joking wif u oni...',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Z4OuCuWDe7qI",
    "outputId": "8d0fe1d7-5277-42e2-b253-4744d1eec73c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxKsfHIOe7qM"
   },
   "source": [
    "<br>\n",
    "\n",
    "## Tokenizing and padding text  \n",
    "\n",
    "\n",
    "- We will use the Keras tokenizer to convert each SMS text into a sequence of words, and then create the vocabulary using the **fit_on_texts()** method on the tokenizer.\n",
    "- We then convert the SMS messages to a sequence of integers using the **texts_to_sequences()**. Finally, since the network can only work with fixed length sequencesof integers, we call the **pad_sequences()** function to pad the shorter SMS messages with zeros.\n",
    "- The longest SMS message in our dataset has 189 tokens (words). In many applications where there may be a few outlier sequences that are very long, we would restrict the length to a smaller number by setting the maxlen flag. In that case, sentences longer than maxlen tokens would be truncated, and sentences shorter than maxlen tokens would be padded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "s_Wgmft5e7qN",
    "outputId": "8836cefb-1c35-4088-b25b-ae0c8e9db61a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574 sentences, max length: 189\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "text_sequences = tokenizer.texts_to_sequences(texts) # convert SMS to integers\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences) # pad the shorter SMS with zeros.\n",
    "\n",
    "num_records = len(text_sequences)\n",
    "max_seqlen = len(text_sequences[0]) # The length of all the text sequences is 189\n",
    "\n",
    "print(f\"{num_records:d} sentences, max length: {max_seqlen:d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "EqaLrdY9e7qR",
    "outputId": "0e0c8028-54e0-4b71-85da-a09a37867273"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,   47,  489,    8,   19,    4,  797,  901,\n",
       "          2,  176, 1941, 1105,  659, 1942, 2331,  261, 2332,   71, 1941,\n",
       "          2, 1943,    2,  337,  489,  555,  960,   73,  391,  174,  660,\n",
       "        392, 2997], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ZMbS8nQe7qX"
   },
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Note:\n",
    "> We will also convert our labels to categorical or one-hot encoding format, because the loss function we would like to choose *(categorical cross-entropy)* expects to see the labels in that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83VBz7KVe7qY"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2 # There are only two categories, spam or ham\n",
    "\n",
    "cat_labels = tf.keras.utils.to_categorical(labels, \n",
    "                                           num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "E89coObxe7qc",
    "outputId": "087a2233-574c-44a0-8c80-d0674746e074"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dinJ7f-Te7qg"
   },
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "The tokenizer allows access to the vocabulary created through the word_index attribute, which is basically a dictionary of vocabulary words to their index positions in the vocabulary. We also build the reverse index that enables us to go from index position to the word itself. In addition, we create entries for the PAD character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Cj7r1AMMe7qh",
    "outputId": "a18cd1ff-5919-411e-da81-7fd16d763e7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 9010\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word[0] = \"PAD\"\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"vocab size: {vocab_size:d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A84cFQdpe7qm"
   },
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Create dataset object\n",
    "Finally, we create the dataset object that our network will work with. The dataset object allows us to set up some properties, such as the batch size, declaratively. <br>\n",
    "Here, we build up a dataset from our padded sequence of integers and categorical labels, shuffle the data, and split it into training, validation, and test sets.<br> \n",
    "Finally, we set the batch size for each of the three datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nm00lcdFe7qo"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "\n",
    "dataset = dataset.shuffle(10000)\n",
    "\n",
    "test_size = num_records // 4\n",
    "val_size = (num_records - test_size) // 10\n",
    "\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DvG9luXe7qt"
   },
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Building the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6W-AACrGe7qt"
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(sequences, word2idx, embedding_dim, embedding_file):\n",
    "    EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)\n",
    "    else:\n",
    "        vocab_size = len(word2idx)\n",
    "        E = np.zeros((vocab_size, embedding_dim))\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.word_vec(word)\n",
    "            except KeyError:   # word not in embedding\n",
    "                pass\n",
    "            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n",
    "            #     pass\n",
    "        np.save(embedding_file, E)\n",
    "    return E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-whaFdUe7qx"
   },
   "source": [
    "#### Observation\n",
    "- In order to keep our model size small, we want to only consider embeddings for words that exist in our vocabulary. This is done using the above code, which creates a smaller embedding matrix for each word in the vocabulary. Each row in the matrix corresponds to a word, and the row itself is the vector corresponding to the embedding for the word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "WWvw4Oawe7qy",
    "outputId": "5f3976a6-cd8b-4f40-813d-ed91911b24d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix: (9010, 300)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
    "\n",
    "E = build_embedding_matrix(text_sequences, \n",
    "                           word2idx, \n",
    "                           EMBEDDING_DIM, \n",
    "                           EMBEDDING_NUMPY_FILE)\n",
    "\n",
    "print(\"Embedding matrix:\", E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PP-6sU_Qj3YP"
   },
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Define spam filter\n",
    "\n",
    "Depending on the run mode, that is, whether we will learn the embeddings from scratch, do transfer learning, or do fine-tuning, the Embedding layer in the network would be slightly different. \n",
    "\n",
    "- When the network starts with randomly initialized embedding weights (run_mode == \"scratch\"), and learns the weights during the training, we set the trainable parameter to True. \n",
    "\n",
    "- In the transfer learning case (run_mode == \"vectorizer\"), we set the weights from our embedding matrix E but set the trainable parameter to False, so it doesn't train.\n",
    "\n",
    "- In the fine-tuning case (run_mode == \"finetuning\"), we set the embedding weights from our external matrix E (in the data folder), as well as set the layer to trainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ToNOAfde7q2"
   },
   "outputs": [],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_sz, embed_sz, input_length, num_filters, kernel_sz, output_sz, run_mode, embedding_weights, **kwargs):\n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        if run_mode == \"scratch\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                                                       embed_sz,\n",
    "                                                       input_length=input_length,\n",
    "                                                       trainable=True)\n",
    "        elif run_mode == \"vectorizer\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                                                       embed_sz,\n",
    "                                                       input_length=input_length,\n",
    "                                                       weights=[embedding_weights],\n",
    "                                                       trainable=False)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
    "                                                       embed_sz,\n",
    "                                                       input_length=input_length,\n",
    "                                                       weights=[embedding_weights],\n",
    "                                                       trainable=True)\n",
    "            \n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
    "                                         kernel_size=kernel_sz,\n",
    "                                         activation=\"relu\")\n",
    "      \n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "      \n",
    "        self.dense = tf.keras.layers.Dense(output_sz, \n",
    "                                         activation=\"softmax\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YA3pBg65ALJI"
   },
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "### What is Fine tuning?\n",
    "- Fine tuning is a process to take a network model that has already been trained for a given task, and make it perform a second similar task.<br>\n",
    "Assuming the original task is similar to the new task, using a network that has already been designed and trained allows us to take advantage of the feature extraction that happens in the front layers of the network without developing that feature extraction network from scratch. Fine tuning:\n",
    "  - Replaces the output layer, originally trained to recognize (in the case of imagenet models) 1,000 classes, with a layer that recognizes the number of classes you require\n",
    "  - The new output layer that is attached to the model is then trained to take the lower level features from the front of the network and map them to the desired output classes, using SGD\n",
    "  - Once this has been done, other late layers in the model can be set as 'trainable=True' so that in further SGD epochs their weights can be fine-tuned for the new task too.\n",
    "\n",
    "The above information is short description of a the [resource](http://wiki.fast.ai/index.php/Fine_tuning)<br>\n",
    "\n",
    "For getting more information check this [blog](https://blog.revolutionanalytics.com/2016/08/deep-learning-part-2.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "colab_type": "code",
    "id": "Os4ixlSXe7q6",
    "outputId": "b5b88866-d2e0-4840-b1d7-6e1269ce2bb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"spam_classifier_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  2703000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              multiple                  230656    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  514       \n",
      "=================================================================\n",
      "Total params: 2,934,170\n",
      "Trainable params: 231,170\n",
      "Non-trainable params: 2,703,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_num_filters = 256\n",
    "conv_kernel_size = 3\n",
    "\n",
    "# (run_mode == \"scratch\")    => Learns the weights during the training.\n",
    "# (run_mode == \"vectorizer\") => Transfer learning\n",
    "# (run_mode == \"finetuning\") => Fine tuning \n",
    "run_mode='vectorizer'\n",
    "\n",
    "\n",
    "\n",
    "model = SpamClassifierModel(vocab_size, \n",
    "                            EMBEDDING_DIM, \n",
    "                            max_seqlen, # 189 words\n",
    "                            conv_num_filters, \n",
    "                            conv_kernel_size, \n",
    "                            NUM_CLASSES,\n",
    "                            run_mode, E)\n",
    "\n",
    "model.build(input_shape=(None, max_seqlen))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngOKpPWznxdZ"
   },
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQGCud6ke7q-"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4k93kHan0pB"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "9mAYkgvAe7rC",
    "outputId": "6db00439-9bfe-4780-fb2c-247a600ecf49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 0.4724 - accuracy: 0.8726 - val_loss: 0.1445 - val_accuracy: 0.9609\n",
      "Epoch 2/3\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 0.2141 - accuracy: 0.9607 - val_loss: 0.0709 - val_accuracy: 0.9792\n",
      "Epoch 3/3\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 0.1437 - accuracy: 0.9725 - val_loss: 0.0437 - val_accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7e242bb470>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_WEIGHTS = { 0: 1, 1: 8 }\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "model.fit(train_dataset, \n",
    "          epochs=NUM_EPOCHS, \n",
    "          validation_data=val_dataset, \n",
    "          class_weight=CLASS_WEIGHTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dIyO0Xgni3z"
   },
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "25UH2TNOe7rJ",
    "outputId": "7d0df011-744b-4060-b0c9-d848af245578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 1.000\n",
      "confusion matrix\n",
      "[[1123    0]\n",
      " [   0  157]]\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ytest, axis=1)\n",
    "    ytest_ = np.argmax(Ytest_, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest.tolist())\n",
    "\n",
    "print(f\"test accuracy: {accuracy_score(labels, predictions):.3f}\")\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mKK5Otl5oKSp"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "02_word_embedding_for_spam_detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
